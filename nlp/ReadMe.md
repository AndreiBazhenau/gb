# –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∫—É—Ä—Å–∞ "–í–≤–µ–¥–µ–Ω–∏–µ –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞"

- **bert_token.py** 
- **corpus** - —Ñ–∞–π–ª –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤

[–£—Ä–æ–∫ 1 –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞](https://gb.ru/lessons/142818)

- **01_word_embeddings.pdf** - –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –®–ê–î Word Embeddings - https://lena-voita.github.io/nlp_course/word_embeddings.html
- **lesson_1_v01.ipynb**
  - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ RE (regex)
  - –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
  - –í–≤–µ–¥–µ–Ω–∏–µ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É NTLK ((Natural Language Toolkit)
    - —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –∏ –ø–æ —Å–ª–æ–≤–∞–º
    - –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥: MorphAnalyzer
    - —Å—Ç–æ–ø-—Å–ª–æ–≤–∞: stopwords –∏–∑ nltk.corpus
  - –ü—Ä–∏–º–µ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

[–£—Ä–æ–∫ 2 –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞](https://gb.ru/lessons/142819)

- **lesson_2.pptx** - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ - –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
  - –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
  - –ú–µ—à–æ–∫ —Å–ª–æ–≤ BoW
  - –ú–µ—à–æ–∫ —Å–ª–æ–≤ CountVectorizer
  - –ú–µ—à–æ–∫ —Å–ª–æ–≤ TF-IDF
  - Pointwise Mutual Information (PMI)
  - –≠–º–±–µ–¥–∏–Ω–≥–∏ —Å–ª–æ–≤
- **lesson_2_actual.ipynb**
  - –ú–µ—à–æ–∫ —Å–ª–æ–≤ - bag of words
  - –ú–µ—à–æ–∫ N-–≥—Ä–∞–º–º: ngams –∏–∑ ntlk.utils
  - TF-IDF (term frequency ‚Äî inverse document frequency)
  - –í–µ–∫—Ç–æ—Ä–∞–π–∑–µ—Ä HashingVectorizer

[–£—Ä–æ–∫ 3 Embedding word2vec fasttext](https://gb.ru/lessons/142820)

- **lesson_3.ipynb**
  - –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤ (word embeddings)
  - word2vec
  - Simple chat-bot example
- **lesson_3_word_embeddings.ipynb**
  - Word embeddings
    - One-hot encodings
    - –ö–æ–¥–∏—Ä–æ–≤–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º
    - Word embeddings
  - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ª–æ—è Embedding
  - Text preprocessing
  - Classification model
- **lesson_3_word2vec.ipynb**
  - Word2Vec
  - Skip-gram and Negative Sampling
  - –í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞

[–£—Ä–æ–∫ 4 –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ. EM-–∞–ª–≥–æ—Ä–∏—Ç–º](https://gb.ru/lessons/142821)

- **lesson_4_Embeddings.pptx** - –≠–º–±–µ–¥–¥–∏–Ω–≥–∏. Word2vec, Doc2Vec, fasttext
- **lesson_4_Topic_model.pptx** - –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

- **lesson_4.ipynb**
  - Word2vec (cbow - contnious Bag-of-words model & skipgram) & Fasttext
  - –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
  - –†–∞–∑–º–µ—Ç–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    - –†–∞–∑–º–µ—Ç–∫–∞ —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ (Parts-Of-Speech)
    - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ POS —Ç—ç–≥–≥–µ—Ä–æ–≤
    - –†–∞–∑–º–µ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–æ–≤
  - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
  - SpaCy - –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–æ–≥ NTLK
  - DeepPavlov
  - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ (Relation extraction)
  - Allennlp - NLP library, built on PyTorch.
- **lesson_4_topic_modeling.ipynb**
  - –ú–æ–¥–µ–ª—å LDA - –ª–∞—Ç–µ–Ω—Ç–Ω–æ–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –î–∏—Ä–∏—Ö–ª–µ. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ gensim
  - pyLDAvis - –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–º –Ω–∞ –ø–ª–æ—Å–∫–æ—Å—Ç–∏
  - –ü–µ—Ä–ø–ª–µ–∫—Å–∏—è –∏ –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å
  - BigARTM
    - –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ –≤ BigARTM
    - –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
- **lesson_4_v01.ipynb** - –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ

[–£—Ä–æ–∫ 5 Part-of-Speech —Ä–∞–∑–º–µ—Ç–∫–∞, NER, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π](https://gb.ru/lessons/142822)

- **lesson_5.ipynb** - –†–∞–∑–º–µ—Ç–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
  - –†–∞–∑–º–µ—Ç–∫–∞ —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å—é —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ (Parts-Of-Speech)
    -  –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ç–µ–≥–æ–≤ —á–∞—Å—Ç–∏ —Ä–µ—á–∏
    - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ POS —Ç—ç–≥–≥–µ—Ä–æ–≤
    - DefaultTagger
    - UnigramTagger
    - BigramTagger
    - TrigramTagger
    - RegexpTagger
    - –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —Ç—ç–≥–≥–µ—Ä–æ–≤
    - –°–æ–∑–¥–∞–Ω–∏–µ —Ç—ç–≥–≥–µ—Ä–∞ –∏–º–µ–Ω
    - –†–∞–∑–º–µ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–æ–≤
  - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π (NER) –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
    - NLTK
    - Spacy
    - Deeppavlov
  - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
    - Spacy
    - NLTK
    - Allennlp

[–£—Ä–æ–∫ 6 –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞. –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞](https://gb.ru/lessons/142823)

- **lesson_6.ipynb**
  - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    - –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    - –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    - Feature Engineering
    - Count Vectors
    - TF-IDF –≤–µ–∫—Ç–æ—Ä–∞
    - Text / NLP –ø—Ä–∏–∑–Ω–∞–∫–∏
  - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    - Naive Bayes
    - Linear Classifier (–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è)
    - SVM
    - RandomForest
    - Boosting Model
  - –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
  - Data Visualization - –æ–±–ª–∞–∫–æ —Å–ª–æ–≤
  - –ü—Ä–∏–º–µ—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º
- **lesson_6_colab_text_classification_part1.ipynb** - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤ —Å IMDB –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ - –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞.
  - –ü—Ä–∏–¥—É–º—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    - Tf-idf
    - N-–≥—Ä–∞–º–º—ã —Å–ª–æ–≤
    - N-–≥—Ä–∞–º–º—ã —Å–∏–º–≤–æ–ª–æ–≤
  - –ü–æ–¥–∫–ª—é—á–∞–µ–º –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫—É
    - –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥
    - NER - –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
    - –í–∫–ª—é—á–∞–µ–º –¥–∏–ø –ª—ë—Ä–Ω–∏–Ω–≥
- **lesson_6_colab_text_classification_part2.ipynb** - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤ —Å IMDB
- **lesson_6_colab_text_classification_part3.ipynb** - –¢–µ–∫—Å—Ç—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤—ã–ø–æ–ª–Ω–∏–ª–∏: –í–∏–ª—å—è–º –ö—É–ø–µ—Ä, –≠—Ä–ª –≠–¥–≤–∞—Ä–¥, –°—ç–º—é–µ–ª—å –ë–∞—Ç–ª–µ—Ä.


[–£—Ä–æ–∫ 7 –°–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞](https://gb.ru/lessons/142824)

- **lesson_7_dl-nlp-cnn.ipynb**
  - –û—Å–Ω–æ–≤–Ω—ã–µ –±–ª–æ–∫–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π:
    - –°–≤—ë—Ä—Ç–æ—á–Ω—ã–µ –±–ª–æ–∫–∏ (–¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤)
    - –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –±–ª–æ–∫–∏
    - –ë–ª–æ–∫–∏ –ø—É–ª–∏–Ω–≥–∞
    - –ë–ª–æ–∫–∏ –≤–Ω–∏–º–∞–Ω–∏—è
    - –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
    - –ì—Ä–∞—Ñ–æ–≤—ã–µ —Å–≤—ë—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
  - Text classification using CNN
    - Sentiment Analysis
      - Keras model
      - tf-idf –∏ CountVectorizer - —Ö–æ—Ä–æ—à–∏–µ –±–µ–π–∑–ª–∞–π–Ω—ã –∏–∑ –∫–æ—Ä–æ–±–∫–∏
      - Word@Vec —Å —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ–º –∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π

[–£—Ä–æ–∫ 8 –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ RNN LSTM GRU](https://gb.ru/lessons/142825)

- **lesson_8_rnn.pptx** - –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏
  - –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: –∑–∞–¥–∞—á–∏ –∏ –¥–∞–Ω–Ω—ã–µ
  - –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏: RNN, LSTM, GRU
  - –Ø—á–µ–π–∫–∏, —Å–ª–æ–∏ –∏ —Å–≤–æ–π—Å—Ç–≤–∞
  - –ü—Ä–æ–±–ª–µ–º–∞ Batch Normalization –¥–ª—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π
  - Layer Normalization
  - Weight Normalization
  - Perplexity –∏ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
- **lesson_8_rnn.ipynb**
  - –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —à–∞–≥–æ–≤ –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ–∫—Ç—Å—ã –∏ —Å—Ç—Ä–æ–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: 
    - —É–¥–∞–ª–∏—Ç—å –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    - —É–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    - –ø—Ä–∏–≤–µ—Å—Ç–∏ —Å–ª–æ–≤–∞ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ (—Å—Ç–µ–º–º–∏–Ω–≥ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)
    - –ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –º—ã –º–æ–∂–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –Ω–∞—à —Ç–µ–∫—Å—Ç (–Ω–∞–±–æ—Ä —Å–ª–æ–≤) –≤ –≤–∏–¥–µ –≤–µ–∫—Ç–æ—Ä–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏:
      - CounterEncoding
      - HashingVectorizer
      - TfIdfVectorizer
  - –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏:
    - RNN
    - LSTM
    - GRU
- **lesson_9_rnn.ipynb** - Recurrent Neural Networks (RNN) with Keras
  - simple example RNN in Keras
  - RNN layers and RNN cells
  - Cross-batch statefulness
  - RNN State Reuse
  - Bidirectional RNNs
  - Performance optimization and CuDNN kernels
    - Using CuDNN kernels when available
  - RNNs with list/dict inputs, or nested inputs
  - Build a RNN model with nested input/output

[–£—Ä–æ–∫ 9 –Ø–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ](https://gb.ru/lessons/142826)

- **lesson_9_text_generation.ipynb** - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ TensorFlow Keras RNN (–ï–≤–≥–µ–Ω–∏–π –û–Ω–µ–≥–∏–Ω)

[–£—Ä–æ–∫ 10 –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥. –ú–æ–¥–µ–ª—å seq2seq –∏ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è](https://gb.ru/lessons/142827)
- **lesson_10_nmt_colab.ipynb** - –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ http://www.manythings.org/anki/
- **lesson_10_nmt_with_attention_colab.ipynb** - –ú–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º

[–£—Ä–æ–∫ 11 –ú–æ–¥–µ–ª—å Transformer-1](https://gb.ru/lessons/142828)

- **lesson_11_transformer_actual.ipynb** - Transformer model for language understanding
  - Transformer —Å–æ–∑–¥–∞–µ—Ç —Å—Ç–æ–ø–∫–∏ —Å–ª–æ–µ–≤ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è - ¬´ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ¬ª –∏ ¬´ –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ¬ª .
  - Positional encoding
  - Masking
  - Scaled dot product attention
  - Multi-head attention
  - Point wise feed forward network - –°–µ—Ç—å —Å —Ç–æ—á–µ—á–Ω–æ–π –ø—Ä—è–º–æ–π —Å–≤—è–∑—å—é –∏–∑ –¥–≤—É—Ö –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–µ–≤ —Å –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π ReLU –º–µ–∂–¥—É –Ω–∏–º–∏.
  - Encoder layer and decoder layer
  - Transformer

[–£—Ä–æ–∫ 12 –ú–æ–¥–µ–ª—å Transformer-2](https://gb.ru/lessons/142829) - **Text Summarization**

- **lesson_12_summarization_new.ipynb** - Text Summarization. –ó–∞–¥–∞—á–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ "—Å–∂–∞—Ç—ã—Ö" –Ω–æ–≤–æ—Å—Ç–µ–π. –î–∞—Ç–∞—Å–µ—Ç: gazeta.ru
  - Lead-3 baseline - –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ summary.
  - –ú–µ—Ç—Ä–∏–∫–∏ ROUGE-N –∏ ROUGE-L
  - TextRank - unsupervised –º–µ—Ç–æ–¥ –¥–ª—è —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∞—Ç–∫–∏—Ö –≤—ã–∂–∏–º–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞.
  - Navec - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞  –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (–ª—É—á—à–µ –∏ –±—ã—Å—Ç—Ä–µ–µ —á–µ–º RusVectors) –∏–∑ Natasha. Navec = large Russian text datasets + vanila GloVe + quantization
  - Lexrank - –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Å–∞–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
  - LSA - –ª–∞—Ç–µ–Ω—Ç–Ω–æ-—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (*Latent semantic analysis*)
  - Summa - TextRank implementation
  - Oracle summary
  - Attention

[–£—Ä–æ–∫ 13 –ú–æ–¥–µ–ª—å BERT –∏ GPT](https://gb.ru/lessons/142830)

- **lesson_13_BertOverview.ipynb**
  - BERT - Bidirectional Encoder Representations from Transformers - –Ω–∞ –≤—Ö–æ–¥ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –±—É–¥–µ–º –ø–æ–¥–∞–≤–∞—Ç—å —Ñ—Ä–∞–∑—ã, –≤ –∫–æ—Ç–æ—Ä—ã—Ö 15% —Å–ª–æ–≤ –∑–∞–º–µ–Ω–∏–º –Ω–∞ [MASK], –∏ –æ–±—É—á–∏–º –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —ç—Ç–∏ –∑–∞–∫—Ä—ã—Ç—ã–µ –º–∞—Å–∫–æ–π —Å–ª–æ–≤–∞.
    - –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π –±–µ—Ä—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
    - –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π –±–µ—Ä—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ 2 —Ñ—Ä–∞–∑
    - Fine-tuning bert
- **lesson_13_GPT2.ipynb**
  - –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã
    - BPE - Byte Pair Encoding
    - GPT 2: GPT —Ö–æ—Ä–æ—à–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç. BERT —Ö–æ—Ä–æ—à–æ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏, –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ –∑–∞ —Ç–µ–∫—Å—Ç.
    - –ê–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä
    - Huggingface's transformers
- **lesson_13_Question_Answering_with_a_Fine_Tuned_BERT.ipynb**
  - DeepPavlov RuBERT –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã. –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ SberQuAD 

[–£—Ä–æ–∫ 14 Transfer learning](https://gb.ru/lessons/142831)

- **lesson_14_GPT_example.ipynb** - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑ GPT —Ç–≤–∏—Ç–æ–≤ –ø–æ –ø—Ä–∏–º–µ—Ä–∞–º –∏–∑ –∞–∫–∫–∞—É–Ω—Ç–∞ "–£—Å—ã –ü–µ—Å–∫–æ–≤–∞" –∏ –¥–∏–∞–ª–æ–≥–æ–≤ –∫–∞–∫ –Ω–∞ bash.org.ru
- **lesson_14_transformers.ipynb** - transformers Huggingface
  - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö transformers
  - –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
  - –≠–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å - —ç—Ç–æ –∑–∞–¥–∞—á–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –∑–∞–¥–∞–Ω –≤–æ–ø—Ä–æ—Å. –ü—Ä–∏–º–µ—Ä–æ–º –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —è–≤–ª—è–µ—Ç—Å—è –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö SQuAD
  - –Ø–∑—ã–∫–æ–≤–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
  - Fine-tuning with Keras
  - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ huggingface –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–∞–∫ —ç–º–±–µ–¥–∏–Ω–≥–∏.
- **lesson_14_BERT_NER.ipynb** - –Ω–æ—É—Ç–±—É–∫ –∏–∑ Hugging Face —Å –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è–º–∏
  - Fine-tuning a model on a token classification task: how to fine-tune one of the Hugging Face ü§ó Transformers model to a token classification task, which is the task of predicting a label for each token. The most common token classification tasks are:
    - NER (Named-entity recognition) Classify the entities in the text (person, organization, location...).
    - POS (Part-of-speech tagging) Grammatically classify the tokens (noun, verb, adjective...)
    - Chunk (Chunking) Grammatically classify the tokens and group them into "chunks" that go together
  - –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
- **lesson_14_simple_bot.ipynb**

[–£—Ä–æ–∫ 15 –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è –ø–æ –∫—É—Ä—Å–æ–≤–æ–º—É –ø—Ä–æ–µ–∫—Ç—É. –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Ç-–±–æ—Ç–∞ –≤ Telegram](https://gb.ru/lessons/142832)

- **lesson_15_chat-bot.ipynb** - —Å–æ–∑–¥–∞–Ω–∏–µ telegram —á–∞—Ç-–±–æ—Ç–∞ –Ω–∞ DialogFlow –∏–ª–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ telegram.ext.

 - **lesson_15_.ipynb** - —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ–¥–±–∏—Ä–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤ —Å –Æ–ª—ã, –∞ –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –æ–¥–µ–∂–¥–µ –±–æ—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å. –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞:
   - Otvety.txt - –æ—Ç–≤–µ—Ç—ã —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ —Å mail.ru
   - ProductsDataset.csv - —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –æ–¥–µ–∂–¥—ã —Å –Æ–ª—ã

[–£—Ä–æ–∫ 16 –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è –ø–æ –∫—É—Ä—Å–æ–≤–æ–º—É –ø—Ä–æ–µ–∫—Ç—É. –°–æ–∑–¥–∞–Ω–∏–µ —á–∞—Ç-–±–æ—Ç–∞ –≤ Telegram](https://gb.ru/lessons/142833)

 - **lesson_16_GPT_tune.ipynb** - –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ [Colab](https://colab.research.google.com/drive/1gJtv5s9K66DS9iOabfSdHb5c8kNYGU-O) - GPT SberAI –æ–±—É—á–∞—è—Å—å –Ω–∞ —Ä–µ—Ü–µ–ø—Ç–∞—Ö, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ—Ü–µ–ø—Ç—ã –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ñ—Ä–∞–∑–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ—é—â–∏—Ö—Å—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤.
 - **lesson_16_GPT_tune1.ipynb** - –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ —Ä–µ—Ü–µ–ø—Ç–æ–≤ —Å Kaggle –∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ SberAI rugpt3small_based_on_gpt2 –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ—Ü–µ–ø—Ç—ã –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ñ—Ä–∞–∑–µ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏–º–µ—é—â–∏—Ö—Å—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤.
 - **lesson_16_Bert_NER_ru.ipynb** - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ª–µ–∫–∞—Ä—Å—Ç–≤ –ø–æ —Å–∏–º–ø—Ç–æ–º–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–∑—ã–≤–æ–≤ —Å —Å–∞–π—Ç–∞ –æ—Ç–∑–æ–≤–∏–∫. 
    - –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤—ã–±—Ä–∞–Ω —Å–∞–º—ã–π –º–∞–ª–µ–Ω—å–∫–∏–π BERT –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ rubert-tiny.
    - –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: –ø–∞–π–ø–ª–∞–π–Ω –æ—Ç huggingface.
 - **lesson_16_BERT_toxicity_classification.ipynb** - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤/–ø–æ—Å—Ç–æ–≤/—Ç–≤–∏—Ç–æ–≤/—Ç–µ–∫—Å—Ç–æ–≤ –∫–∞–∫ —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∏–ª–∏ –Ω–µ–ø–æ–ª–∏—Ç–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ huggingface ruber-tiny https://huggingface.co/cointegrated/rubert-tiny-toxicity –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞—è, –æ–±—É—á–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –ø–æ 5 –∫–ª–∞—Å—Å–∞–º:
    - non-toxic
    - insult
    - obscenity
    - threat
    - dangerous

