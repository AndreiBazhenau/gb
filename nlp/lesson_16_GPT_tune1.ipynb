{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-tune.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMLBBHTUm4k7"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt_kd0d3nmbk"
      },
      "source": [
        "# загрузим данные\n",
        "\n",
        "https://www.kaggle.com/coolonce/recipes-and-interpretation-dim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV06C2K2oFir",
        "outputId": "9ec948ed-1b66-42ae-9da6-3efaab8d82d6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbZupKNxoG3e"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkFTYtlipaJP"
      },
      "source": [
        "df_rec = pd.read_csv('all_recepies_inter.csv', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXrVJIR7phZo"
      },
      "source": [
        "df_rec[:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcZVnBGOqbAK"
      },
      "source": [
        "print(df_rec.loc[0, 'Инструкции'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7MqR-Xqq9C-"
      },
      "source": [
        "print(df_rec.loc[10, 'Инструкции'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJzTsW6v5wAM"
      },
      "source": [
        "df_rec.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quYHZlfVuA85"
      },
      "source": [
        "data = df_rec.loc[:5000, 'Инструкции']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEduuADNrOMV"
      },
      "source": [
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_text_files(data_json, dest_path):\n",
        "    f = open(dest_path, 'w')\n",
        "    data = ''\n",
        "    for texts in data_json:\n",
        "        summary = str(texts).strip()\n",
        "        summary = re.sub(r\"\\s\", \" \", summary)\n",
        "        data += summary + \"  \"\n",
        "    f.write(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDkqY7Bux7HK"
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.15)\n",
        "\n",
        "build_text_files(train,'train_dataset.txt')\n",
        "build_text_files(test,'test_dataset.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGfe1uqryv6Y",
        "outputId": "a3b2ec87-908b-4bfa-9bf8-9eb6af588c89"
      },
      "source": [
        "print(\"Train dataset length: \"+ str(len(train)))\n",
        "print(\"Test dataset length: \"+ str(len(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dataset length: 4250\n",
            "Test dataset length: 751\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI2qQ4p6y4__"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "#sberbank-ai/rugpt3medium_based_on_gpt2\n",
        "#sberbank-ai/rugpt3large_based_on_gpt2\n",
        "#sberbank-ai/rugpt3small_based_on_gpt2\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
        "\n",
        "train_path = 'train_dataset.txt'\n",
        "test_path = 'test_dataset.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYHGMuk20c_d"
      },
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(train_path, test_path, tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)\n",
        "\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slTvi69p1XP6"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqsE0ja61UnI"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-sv\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ER9m-321F4s"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOiGfCO12zD2"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyaAcL18MAtl"
      },
      "source": [
        "trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY3PucLaR0t0"
      },
      "source": [
        "prefix = \"брем свежие томаты \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2e3JWnERoJs"
      },
      "source": [
        "tokens = tokenizer(prefix, return_tensors='pt')\n",
        "tokens = {k: v.to(model.device) for k, v in tokens.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU5glEt3Q30k"
      },
      "source": [
        "size = tokens['input_ids'].shape[1]\n",
        "output = model.generate(\n",
        "    **tokens, \n",
        "    #end_token=end_token_id,\n",
        "    do_sample=False, \n",
        "    max_length=size+50, \n",
        "    repetition_penalty=5., \n",
        "    temperature=0.5,\n",
        "    num_beams=10,\n",
        ")\n",
        "\n",
        "decoded = tokenizer.decode(output[0])\n",
        "result = decoded[len(prefix):]\n",
        "print(prefix + result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLydTTZmTBBQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}