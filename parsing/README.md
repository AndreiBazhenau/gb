# Проекты по сбору и обработке данных  

**По курсу выполнены следующие проекты**:



**1.api**: **Основы клиент-серверного взаимодействия. Парсинг API**. 

1) Посмотреть документацию к API GitHub, разобраться как вывести список репозиториев для конкретного пользователя, сохранить JSON-вывод в файле *.json.

2) Изучить список открытых API. Найти среди них любое, требующее авторизацию (любого типа). Выполнить запросы к нему, пройдя авторизацию. Ответ сервера записать в файл. *Выполнено на примере VK*.

**2.soup**: **Парсинг HTML. BeautifulSoup, MongoDB**.

Собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайта hh.ru. Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум: 

- Наименование вакансии; 
- Предлагаемую зарплату (отдельно мин. и отдельно макс.); 
- Ссылку на саму вакансию; 
- Сайт откуда собрана вакансия.

**3.mongo_sql**: **Системы управления базами данных MongoDB и SQLite в Python**.

1) Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, записывающую собранные вакансии в созданную БД
2) Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введенной суммы.

**4.xpath**: **Парсинг HTML. XPath**.

1) Написать приложение, которое собирает основные новости с сайтов news.mail.ru, lenta.ru, yandex.news
Для парсинга использовать xpath. Структура данных должна содержать:

- название источника,
- наименование новости,
- ссылку на новость,
- дата публикации

2) Сложить все новости в БД MongoDB.

**5.selenium** 

1) Написать программу, которая собирает входящие письма из почтового ящика mail.ru и сложить данные о письмах в базу данных MongoDB:

- от кого,
- дата отправки,
- тема письма,
- текст письма полный

3) Написать программу, которая собирает «Хиты продаж» с сайта техники mvideo и складывает данные в БД. Магазины можно выбрать свои. Главный критерий выбора: динамически загружаемые товары.

**6.scrapy**

1) Разработать паука в Scrapy проекте, чтобы он формировал item по структуре:

- Наименование вакансии

- Зарплата от

- Зарплата до

- Ссылку на саму вакансию

- Сайт откуда собрана вакансия

Сохранить все записи в БД (любую)

2) Создать в проекте второго паука по сбору вакансий с сайта superjob. Паук должен формировать item'ы по аналогичной структуре и складывать данные также в БД.

**7.scrapy**: **Scrapy. Парсинг фото и файлов**

1) Взять любую категорию товаров на сайте Леруа Мерлен. Собрать с использованием ItemLoader следующие данные:
- название;
- все фото;
- параметры товара в объявлении;
- ссылка;
- цена.

С использованием output_processor и input_processor реализовать очистку и преобразование данных. Цены должны быть в виде числового значения.

2) Написать универсальный обработчик параметров объявлений, который будет формировать данные вне зависимости от их типа и количества.

3)* Реализовать хранение скачиваемых файлов в отдельных папках, каждая из которых должна соответствовать собираемому товару

**8.scrapy**: **Scrapy. Парсинг Instagram**.

1) Написать приложение, которое будет проходиться по указанному списку двух и/или более пользователей Instagam и собирать данные об их подписчиках и подписках.
2) По каждому пользователю, который является подписчиком или на которого подписан исследуемый объект нужно извлечь имя, id, фото (остальные данные по желанию). Фото можно дополнительно скачать.
3) Собранные данные необходимо сложить в базу данных. Структуру данных нужно заранее продумать, чтобы:

- Написать запрос к базе, который вернет список подписчиков только указанного пользователя
- Написать запрос к базе, который вернет список профилей, на кого подписан указанный пользователь