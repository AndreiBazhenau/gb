{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Часть 1. Подготовка датасета","metadata":{"id":"gYzFpUp7REeg"}},{"cell_type":"markdown","source":"В обучении нашей модели мы будем использовать датасет для автопилотируемых машин.","metadata":{"id":"g8mWOs3XREeh"}},{"cell_type":"markdown","source":"Шаг 1.\n\nНачнем с того, что посмотрим на наш датасет. Внутри куча схожих по названию папок, каждая из которых содержит картинки. \nНо мы можем выделить в этом датасете два вида картинок. \n<br>1) Это обычные цветные картинки.\nНапример dataA/dataA/CameraRGB/02_00_000.png\n<br>2) И есть связанные с ними картинки, разбитые на области с одинаковыми яркостями пикселей.\n<br>Например dataA/dataA/CameraSeg/02_00_000.png, в ней все тоже самое, что и в первой, но  она просегментирована.\n\nИ еще заметим, что нигде в датасете нет явной информации о классах. Мы должны дать им имена сами.","metadata":{"id":"KDf7XrPuREeh"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport cv2","metadata":{"id":"bd1cnuhLREei","execution":{"iopub.status.busy":"2021-08-20T21:06:08.607781Z","iopub.execute_input":"2021-08-20T21:06:08.608177Z","iopub.status.idle":"2021-08-20T21:06:08.776487Z","shell.execute_reply.started":"2021-08-20T21:06:08.608087Z","shell.execute_reply":"2021-08-20T21:06:08.775617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Найдем уникальные значения пискселей на картинке, и каждое такое значение будет соответствовать целому классу.","metadata":{"id":"ck26THHXREei"}},{"cell_type":"code","source":"DATA_ROOT = '/kaggle/input/lyft-udacity-challenge/'","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:06:08.778208Z","iopub.execute_input":"2021-08-20T21:06:08.778571Z","iopub.status.idle":"2021-08-20T21:06:08.783842Z","shell.execute_reply.started":"2021-08-20T21:06:08.778533Z","shell.execute_reply":"2021-08-20T21:06:08.782739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = plt.imread(DATA_ROOT + 'dataA/dataA/CameraSeg/02_00_000.png')\nplt.imshow(img[..., 0]);","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:06:08.786238Z","iopub.execute_input":"2021-08-20T21:06:08.78682Z","iopub.status.idle":"2021-08-20T21:06:09.030516Z","shell.execute_reply.started":"2021-08-20T21:06:08.786771Z","shell.execute_reply":"2021-08-20T21:06:09.029668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(img * 255)","metadata":{"id":"iX7tRyMrREei","outputId":"70dd394e-ae6b-4df3-9c54-ad28e32fe593","execution":{"iopub.status.busy":"2021-08-20T21:06:09.032211Z","iopub.execute_input":"2021-08-20T21:06:09.032549Z","iopub.status.idle":"2021-08-20T21:06:09.079266Z","shell.execute_reply.started":"2021-08-20T21:06:09.032513Z","shell.execute_reply":"2021-08-20T21:06:09.078189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В итоге видим, что у нас 13 классов. Вы можете самостоятельно поотображать семантическую маску \nдля каждого класса используя код ниже:","metadata":{"id":"JNwigEd5REek"}},{"cell_type":"code","source":"labels = ['Unlabeled','Building','Fence','Other',\n          'Pedestrian', 'Pole', 'Roadline', 'Road',\n          'Sidewalk', 'Vegetation', 'Car','Wall',\n          'Traffic sign']","metadata":{"id":"au8A5cDcREek","execution":{"iopub.status.busy":"2021-08-20T21:06:09.080972Z","iopub.execute_input":"2021-08-20T21:06:09.081361Z","iopub.status.idle":"2021-08-20T21:06:09.086066Z","shell.execute_reply.started":"2021-08-20T21:06:09.081321Z","shell.execute_reply":"2021-08-20T21:06:09.084851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(13):\n    mask = plt.imread(DATA_ROOT + 'dataA/dataA/CameraSeg/02_00_000.png') * 255\n    mask = np.where(mask == i, 255, 0)\n    mask = mask[:,:,0]\n    print(np.unique(mask))\n    plt.title(f'class: {i} {labels[i]}')\n    plt.imshow(mask)\n    plt.show()","metadata":{"id":"MzOb4_viREel","outputId":"2397e9d6-ac80-4ffd-c08d-37afd6425d3e","execution":{"iopub.status.busy":"2021-08-20T21:06:09.087531Z","iopub.execute_input":"2021-08-20T21:06:09.088214Z","iopub.status.idle":"2021-08-20T21:06:11.638856Z","shell.execute_reply.started":"2021-08-20T21:06:09.088174Z","shell.execute_reply":"2021-08-20T21:06:11.637842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Шаг 2.\n\nТеперь приведем наш датасет к удобному виду, для этого сначала разделим все на два списка с rgb картинками и seg.","metadata":{"id":"wWWW8Cm2REel"}},{"cell_type":"code","source":"cameraRGB = []\ncameraSeg = []\nfor root, dirs, files in os.walk(DATA_ROOT):\n    for name in files:\n        f = os.path.join(root, name)\n        if 'CameraRGB' in f:\n            cameraRGB.append(f)\n        elif 'CameraSeg' in f:\n            cameraSeg.append(f)\n        else:\n            break","metadata":{"id":"w38jntDsREem","execution":{"iopub.status.busy":"2021-08-20T21:06:11.64045Z","iopub.execute_input":"2021-08-20T21:06:11.640851Z","iopub.status.idle":"2021-08-20T21:06:15.517298Z","shell.execute_reply.started":"2021-08-20T21:06:11.640812Z","shell.execute_reply":"2021-08-20T21:06:15.512348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь завернем эти два списка в DataFrame из библиотеки pandas.\nВ итоге выведем первые пять записей из получившегося датафрейма:","metadata":{"id":"K4i7aVsVREem"}},{"cell_type":"code","source":"df = pd.DataFrame({'cameraRGB': cameraRGB, 'cameraSeg': cameraSeg})\n# Отсортируем  датафрейм по значениям\ndf.sort_values(by='cameraRGB',inplace=True)\n\ndf.reset_index(drop=True, inplace=True)\n# Выведем первые пять значений нашего датафрейма\ndf.head(5)","metadata":{"id":"iXEpOHcmREem","outputId":"6e7ec8f3-a1c3-4a37-b311-1f0b6de1eb91","execution":{"iopub.status.busy":"2021-08-20T21:06:15.519884Z","iopub.execute_input":"2021-08-20T21:06:15.520176Z","iopub.status.idle":"2021-08-20T21:06:15.710029Z","shell.execute_reply.started":"2021-08-20T21:06:15.520151Z","shell.execute_reply":"2021-08-20T21:06:15.707219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Шаг 3. \n\nТеперь обернем все в кастомный датасет для удобной работы в PyTorch.","metadata":{"id":"B12iBsZ7REen"}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torch\nfrom torch.nn import functional as F","metadata":{"id":"XReSUZvaREen","execution":{"iopub.status.busy":"2021-08-20T21:06:15.714422Z","iopub.execute_input":"2021-08-20T21:06:15.716407Z","iopub.status.idle":"2021-08-20T21:06:16.966771Z","shell.execute_reply.started":"2021-08-20T21:06:15.71637Z","shell.execute_reply":"2021-08-20T21:06:16.965892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создадим класс для кастомного датасета:","metadata":{"id":"oUm4KLvaREeo"}},{"cell_type":"code","source":"class SelfDrivingDataset(Dataset):\n    def __init__(self, data, preprocessing=None):\n        # Подаем наш подготовленный датафрейм\n        self.data = data\n        \n        # Разделяем датафрейм на rgb картинки \n        self.image_arr = self.data.iloc[:,0]\n        # и на сегментированные картинки\n        self.label_arr = self.data.iloc[:,1]\n        \n        # Количество пар картинка-сегментация\n        self.data_len = len(self.data.index)\n        \n        self.preprocessing = preprocessing\n\n    # переопределяем метод getitem, которым мы достаём объект по индексу\n    def __getitem__(self, index):\n        # Читаем картинку и сразу же представляем ее в виде numpy-массива \n        # размера 600х800 float-значений, кодируя из bgr в rbg\n        img = cv2.cvtColor(cv2.imread(self.image_arr[index]), cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (256, 256))\n\n        img = np.asarray(img).astype('float')\n        \n        if self.preprocessing:\n            img = self.preprocessing(img)\n            img = torch.as_tensor(img)\n        else:\n            # Нормализуем изображение в значениях [0, 1]\n            img = torch.as_tensor(img) / 255.0\n            \n        # приводим к необходимому для торча виду: каналы, ширина, высота\n        img = img.permute(2,0,1)\n        \n        # считаем сегментированную картинку через opencv\n        masks = []\n        mask = cv2.cvtColor(cv2.imread(self.label_arr[index]), cv2.COLOR_BGR2RGB)\n#         # через пиллоу появлялись артефакты\n#         mask = Image.open(self.label_arr[index])\n#         mask = mask.resize((256, 256))\n#         mask = np.asarray(mask)\n        \n        # создаём 13 бинарных масок для 13 классов, чтобы отслеживать\n        # как сеть предсказывает каждый класс и считать метрику по классам\n        for i in range(13):\n            # где маска принимает значение интенсивности пикселя\n            # создаём маску: 255 - где объект есть, 0 - где нет\n            cls_mask = np.where(mask == i, 255, 0)\n            cls_mask = cls_mask.astype('float')\n            cls_mask = cv2.resize(cls_mask, (256, 256))\n            \n            # массив 13-ти масок\n            masks.append(cls_mask[:,:,0] / 255)\n        \n        # переводим в тензор, таб будет 13 каналов для всех масок\n        masks = torch.as_tensor(masks, dtype=torch.uint8)    \n        \n        # возвращаем картинку и предсказание\n        return (img.float(), masks)\n\n    # переопределяем метод подсчёта длины (было определено в init)\n    def __len__(self):\n        return self.data_len","metadata":{"id":"k7yVKefHREeo","execution":{"iopub.status.busy":"2021-08-20T21:06:16.968043Z","iopub.execute_input":"2021-08-20T21:06:16.968372Z","iopub.status.idle":"2021-08-20T21:06:16.982671Z","shell.execute_reply.started":"2021-08-20T21:06:16.968339Z","shell.execute_reply":"2021-08-20T21:06:16.98165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# инициализируем класс датасета, передавая в него датафрейм\ndataset = SelfDrivingDataset(df)\n\n# проверим что всё ок, посмотрев нулевой объект\nimg, masks = dataset[0]\nprint(img.shape, masks.shape)\nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\nax[0].imshow(img.permute(1, 2, 0))\nax[1].imshow(masks.permute(1, 2, 0)[..., 10])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:06:16.984134Z","iopub.execute_input":"2021-08-20T21:06:16.984511Z","iopub.status.idle":"2021-08-20T21:06:17.719411Z","shell.execute_reply.started":"2021-08-20T21:06:16.984477Z","shell.execute_reply":"2021-08-20T21:06:17.718643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"В результате картинка 3 канала 256х256 и 13 масок 256х256","metadata":{}},{"cell_type":"markdown","source":"Затем разделим наш датасет на тренировочную и тестовую выборки.\nИ обернем их в наш кастомный класс.","metadata":{"id":"mTMWIIrRREeo"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 70 % в тренировочную выборку, 30 - в тестовую\nX_train, X_test = train_test_split(df, test_size=0.3)\n\n# Упорядочиваем индексацию\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\n# Оборачиваем каждую выборку в наш кастомный датасет\ntrain_data = SelfDrivingDataset(X_train)\ntest_data = SelfDrivingDataset(X_test)","metadata":{"id":"TEr5WUG5REep","execution":{"iopub.status.busy":"2021-08-20T21:06:17.720524Z","iopub.execute_input":"2021-08-20T21:06:17.72084Z","iopub.status.idle":"2021-08-20T21:06:18.640493Z","shell.execute_reply.started":"2021-08-20T21:06:17.720804Z","shell.execute_reply":"2021-08-20T21:06:18.639691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"И теперь уже обернем то, что получилось в известные нам в pytorch даталоадеры:","metadata":{"id":"VmG_Hu3eREep"}},{"cell_type":"code","source":"train_data_loader = DataLoader(\n    train_data,\n    batch_size=8,\n    shuffle=True\n)\ntest_data_loader = DataLoader(\n    test_data,\n    batch_size=4,\n    shuffle=False # в тесте лучше ничего не перемешивать\n)","metadata":{"id":"0LCPfCUhREep","execution":{"iopub.status.busy":"2021-08-20T21:06:18.641763Z","iopub.execute_input":"2021-08-20T21:06:18.642138Z","iopub.status.idle":"2021-08-20T21:06:18.647642Z","shell.execute_reply.started":"2021-08-20T21:06:18.642083Z","shell.execute_reply":"2021-08-20T21:06:18.646534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# проверим, что генератор исправен, пройдёмся по 1-й итерации\nfor img, target in train_data_loader:\n    print(img.shape, target.shape)\n    print(img[0].min(), img[0].max())\n    print(target[0].min(), target[0].max())\n    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n    ax[0].imshow(img[0].permute(1, 2, 0))\n    ax[1].imshow(target[0].permute(1, 2, 0)[..., 0])\n    break","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:06:18.649304Z","iopub.execute_input":"2021-08-20T21:06:18.649852Z","iopub.status.idle":"2021-08-20T21:06:21.881732Z","shell.execute_reply.started":"2021-08-20T21:06:18.649812Z","shell.execute_reply":"2021-08-20T21:06:21.880933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Получилось 8 - батч-сайз, 3 канала, 256х256 пикселей для картинок\nдля масок 8 -батч-сайз, 13 каналов, 256х256 пикселей.\n\nВидим что картинка была нормализована - изменяется от 0 до 1. Маска тоже.\n\nЕсли оставить о 0 до 255, коэф Dice и IoU будут считаться некорректно. Они ожидают что кратинки и маски будут от 0 до 1. Иначе коэффициенты становятся отрицательными.","metadata":{}},{"cell_type":"markdown","source":"## Часть 2. Создание модели\n\n**Самописный вариант Unet**","metadata":{}},{"cell_type":"markdown","source":"Как мы отметили ранее, в архитектуре присутствует 3х3 двойной сверточный слой следующий за активационной функцией Relu в обеих частях сетки.\n","metadata":{"id":"6qavzR1MREeY"}},{"cell_type":"markdown","source":"Шаг 1.\n\nСоздадим функцию conv_block(), параметры которой входные и выходные параметры каналов. Внутри функции последовательные сверточные слои с ядром 3 (3х3) каждый предшествует Relu активационной функции и для лучшей сходимости слои BatchNorm2d:","metadata":{"id":"XbWJHem0REeY"}},{"cell_type":"markdown","source":"```\nimport torch\nimport torch.nn as nn\n\n# добавляем блок свёрток\ndef conv_block(in_channels,  out_channels):\n    conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3),\n        nn.ReLU(),\n        nn.BatchNorm2d(num_features=out_channels),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3),\n        nn.ReLU(),\n        nn.BatchNorm2d(num_features=out_channels)\n    )\n    return conv\n```","metadata":{}},{"cell_type":"markdown","source":"Шаг 2.\n\nСоздадим класс Unet() и сделаем слои левой части и maxpool слои. В каждом слое мы используем conv_block(). Давайте назовем  слои conv_down (4 слоя в левой части): \n\n\n\n```\nclass Unet(nn.Module):\n    def __init__(self, num_classes):\n        super(Unet, self).__init__()\n        # запоминаем сколько классов\n        self.num_classes = num_classes\n        self.down_conv_11 = conv_block(in_channels=3, out_channels=64)\n        self.down_conv_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_21 = conv_block(in_channels=64, out_channels=128)\n        self.down_conv_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_31 = conv_block(in_channels=128, out_channels=256)\n        self.down_conv_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_41 = conv_block(in_channels=256, out_channels=512)\n        self.down_conv_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.middle = conv_block(in_channels=512, out_channels=1024)\n```\n\n","metadata":{"id":"X3DBNgoGREea"}},{"cell_type":"markdown","source":"Сделаем внутри класса функцию forward(), которой мы отправим входное изображение в левую часть:\n\n\n```\n        def forward(self, X):\n        \n            x1 = self.down_conv_11(X) # [-1, 64, 256, 256]\n            x2 = self.down_conv_12(x1) # [-1, 64, 128, 128]\n            x3 = self.down_conv_21(x2) # [-1, 128, 128, 128]\n            x4 = self.down_conv_22(x3) # [-1, 128, 64, 64]\n            x5 = self.down_conv_31(x4) # [-1, 256, 64, 64]\n            x6 = self.down_conv_32(x5) # [-1, 256, 32, 32]\n            x7 = self.down_conv_41(x6) # [-1, 512, 32, 32]\n            x8 = self.down_conv_42(x7) # [-1, 512, 16, 16]\n            middle_out = self.middle(x8) # [-1, 1024, 16, 16]\n```\n\n","metadata":{"id":"6nggmYaGREeb"}},{"cell_type":"markdown","source":"Вот, отлично. Мы создали левую часть нейронной сети. Осталось сделать правую часть.","metadata":{"id":"ShEu1sDzREec"}},{"cell_type":"markdown","source":"Шаг 4.\n\nТеперь давайте задекларируем 4 слоя правой части и последнюю 1х1 conv в нашей функции __init__() класса. Вместо maxpool функции мы будем использовать 2х2 transpose convolution, которая будет повышать нашу размерность:","metadata":{"id":"1Iwia83wREed"}},{"cell_type":"markdown","source":"```\n        self.up_conv_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512,kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_12 = conv_block(in_channels=1024, out_channels=512)\n        self.up_conv_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_22 = conv_block(in_channels=512, out_channels=256)\n        self.up_conv_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_32 = conv_block(in_channels=256, out_channels=128)\n        self.up_conv_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_42 = conv_block(in_channels=128, out_channels=64)\n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n```\n","metadata":{"id":"YTOwj0CjREed"}},{"cell_type":"markdown","source":"Шаг 5.\n\nКак мы видим, в архитектуре входное изображение в правой части - это комбинация изображения с левой части\nи с предыдущего слоя. Но для комбинации изображений они должны быть одинаковых размеров. Поэтому давайте создадим функцию crop_tensor() для вырезания этих изображений. Внутри этой функции мы подразумеваем, что наши изображения - это тензоры.","metadata":{"id":"gFh3_fEgREed"}},{"cell_type":"markdown","source":"Что происходит в функции crop_tensor() ?\n\ntensor = изображение с левой части, которое необходимо обрезать\ntarget tensor = изображение в правой части, которое сопоставляется с вырезанным левым изображением\n\nВозьмем последний размер обоих тензоров target_size и tensor_size, т.к. их высота и ширина одинаковы. \nНапример: x=torch.Size([1,512,64,64]), таким образом x[2] = 64\n\nТеперь мы имея размеры обоих изображений, вычтем размер меньшего тензора из большего. Предположим\ntarget_size = 56 и tensor_size = 64 -> delta(разница между размерами) будет 8.\n\nНо мы ведь будем вырезать изображение из всех углов 'height' * 'width', поэтому мы разделим delta на 2. \nТаким образом, height и width могут быть вырезаны равно:\n    8 => h * w=4 * 4\n\nтеперь вернем вырезанный тензор\n[:,:,] = все измерения\n[delta:tensor_size-delta, delta:tensor_size-delta] = вырезанное изображение\n\n[4:64-4, 4:64-4] => 4:60, 4:60 \nв примере выше нам необходима картинка 56х56\n\nНа картинке ниже показан пример вырезанной высоты:","metadata":{"id":"CIe1y0GgREee"}},{"cell_type":"markdown","source":"<img src='https://drive.google.com/uc?export=view&id=1AURG8EdTu1OHHj8nxSRhEsrGqc4WNb5V' width=500>","metadata":{"id":"tE746OPrREee"}},{"cell_type":"code","source":"def crop_tensor(target_tensor, tensor):\n    target_size = target_tensor.size()[2]\n    tensor_size = tensor.size()[2]\n    delta = tensor_size - target_size\n    delta = delta // 2\n    \n    return tensor[:,:, delta:tensor_size-delta, delta:tensor_size-delta]","metadata":{"id":"gs04tc1jREee","execution":{"iopub.status.busy":"2021-08-20T21:06:21.883815Z","iopub.execute_input":"2021-08-20T21:06:21.884168Z","iopub.status.idle":"2021-08-20T21:06:21.888925Z","shell.execute_reply.started":"2021-08-20T21:06:21.884134Z","shell.execute_reply":"2021-08-20T21:06:21.888004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Шаг 6.\n\nТеперь допишем наш forward правой части.\n\n\nКомбинируем оба изображения используя torch.cat() и подставляем в up_conv():","metadata":{"id":"AgwzIQOGREef"}},{"cell_type":"markdown","source":"\n\n```\n        x = self.up_conv_11(middle_out) # [-1, 512, 32, 32]\n        y = crop_tensor(x, x7)\n        # конкатенируем и сжимаем\n        x = self.up_conv_12(torch.cat((x, y), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        \n        x = self.up_conv_21(x) # [-1, 256, 64, 64]\n        y = crop_tensor(x, x5)\n        x = self.up_conv_22(torch.cat((x, y), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        \n        x = self.up_conv_31(x) # [-1, 128, 128, 128]\n        y = crop_tensor(x, x3)\n        x = self.up_conv_32(torch.cat((x, y), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        \n        x = self.up_conv_41(x) # [-1, 64, 256, 256]\n        y = crop_tensor(x, x1)\n        x = self.up_conv_42(torch.cat((x, y), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        \n        output = self.output(x) # [-1, num_classes, 256, 256]\n        \n        return output\n```\n\n","metadata":{"id":"PzM-3m0TREef"}},{"cell_type":"markdown","source":"Теперь для вида запишем наши созданые ранее функции внутрь класса. В итоге наш класс Unet выглядит следующим образом:","metadata":{"id":"EOVMcPAmREef"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n\n        # Левая сторона (Путь уменьшения размерности картинки)\n        self.down_conv_11 = self.conv_block(in_channels=3,\n                                            out_channels=64)\n        self.down_conv_12 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        self.down_conv_21 = self.conv_block(in_channels=64,\n                                            out_channels=128)\n        self.down_conv_22 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        self.down_conv_31 = self.conv_block(in_channels=128,\n                                            out_channels=256)\n        self.down_conv_32 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        self.down_conv_41 = self.conv_block(in_channels=256,\n                                            out_channels=512)\n        self.down_conv_42 = nn.MaxPool2d(kernel_size=2,\n                                         stride=2)\n        \n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        \n        # Правая сторона (Путь увеличения размерности картинки)\n        self.up_conv_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_12 = self.conv_block(in_channels=1024,\n                                          out_channels=512)\n        self.up_conv_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_22 = self.conv_block(in_channels=512,\n                                          out_channels=256)\n        self.up_conv_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_32 = self.conv_block(in_channels=256,\n                                          out_channels=128)\n        self.up_conv_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                             kernel_size=3, stride=2,\n                                             padding=1, output_padding=1)\n        self.up_conv_42 = self.conv_block(in_channels=128,\n                                          out_channels=64)\n        \n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes,\n                                kernel_size=3, stride=1,\n                                padding=1)\n        self.softmax = nn.Softmax()\n    \n    @staticmethod\n    def conv_block(in_channels, out_channels):\n        block = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=out_channels,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.Conv2d(in_channels=out_channels,\n                      out_channels=out_channels,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    @staticmethod\n    def crop_tensor(target_tensor, tensor):\n        target_size = target_tensor.size()[2]\n        tensor_size = tensor.size()[2]\n        delta = tensor_size - target_size\n        delta = delta // 2\n\n        return tensor[:,:, delta:tensor_size-delta, delta:tensor_size-delta]\n\n\n    def forward(self, X):\n        # Проход по левой стороне\n        x1 = self.down_conv_11(X) # [-1, 64, 256, 256]\n        x2 = self.down_conv_12(x1) # [-1, 64, 128, 128]\n        x3 = self.down_conv_21(x2) # [-1, 128, 128, 128]\n        x4 = self.down_conv_22(x3) # [-1, 128, 64, 64]\n        x5 = self.down_conv_31(x4) # [-1, 256, 64, 64]\n        x6 = self.down_conv_32(x5) # [-1, 256, 32, 32]\n        x7 = self.down_conv_41(x6) # [-1, 512, 32, 32]\n        x8 = self.down_conv_42(x7) # [-1, 512, 16, 16]\n        \n        middle_out = self.middle(x8) # [-1, 1024, 16, 16]\n\n        # Проход по правой стороне\n        x = self.up_conv_11(middle_out) # [-1, 512, 32, 32]\n        y = self.crop_tensor(x, x7)\n        x = self.up_conv_12(torch.cat((x, y), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        \n        x = self.up_conv_21(x) # [-1, 256, 64, 64]\n        y = self.crop_tensor(x, x5)\n        x = self.up_conv_22(torch.cat((x, y), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        \n        x = self.up_conv_31(x) # [-1, 128, 128, 128]\n        y = self.crop_tensor(x, x3)\n        x = self.up_conv_32(torch.cat((x, y), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        \n        x = self.up_conv_41(x) # [-1, 64, 256, 256]\n        y = self.crop_tensor(x, x1)\n        x = self.up_conv_42(torch.cat((x, y), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        \n        output = self.output(x) # [-1, num_classes, 256, 256]\n        output = self.softmax(output)\n\n        return output","metadata":{"id":"r-U3uNk6REeg","execution":{"iopub.status.busy":"2021-08-20T21:06:21.890662Z","iopub.execute_input":"2021-08-20T21:06:21.891418Z","iopub.status.idle":"2021-08-20T21:06:21.916425Z","shell.execute_reply.started":"2021-08-20T21:06:21.891357Z","shell.execute_reply":"2021-08-20T21:06:21.915551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Часть 3. Обучение","metadata":{"id":"dknM4qTPREeq"}},{"cell_type":"markdown","source":"У нас есть готовые данные и определенная сеть, которую мы хотим обучить. Пришло время построить базовый обучающий конвейер.","metadata":{"id":"nBlGArBbREeq"}},{"cell_type":"markdown","source":"Определим скорость обучения и количество эпох:","metadata":{"id":"wBxyNJioREeq"}},{"cell_type":"code","source":"learning_rate = 0.001\nepochs = 1","metadata":{"id":"9NYzQegDREeq","execution":{"iopub.status.busy":"2021-08-20T21:06:21.918057Z","iopub.execute_input":"2021-08-20T21:06:21.918455Z","iopub.status.idle":"2021-08-20T21:06:21.929537Z","shell.execute_reply.started":"2021-08-20T21:06:21.918418Z","shell.execute_reply":"2021-08-20T21:06:21.928512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Выберем устройство,на котором будем обучать нашу модель:","metadata":{"id":"GVdcV094REer"}},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"CQYqtHIcREer","execution":{"iopub.status.busy":"2021-08-20T21:06:21.931039Z","iopub.execute_input":"2021-08-20T21:06:21.931426Z","iopub.status.idle":"2021-08-20T21:06:22.01742Z","shell.execute_reply.started":"2021-08-20T21:06:21.93139Z","shell.execute_reply":"2021-08-20T21:06:22.016494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определим нашу модель Unet для 13 классов:","metadata":{"id":"O9rrI2WxREer"}},{"cell_type":"code","source":"Umodel = UNet(num_classes=13).to(device)","metadata":{"id":"x6NPeE_jREes","execution":{"iopub.status.busy":"2021-08-20T21:06:22.021091Z","iopub.execute_input":"2021-08-20T21:06:22.021403Z","iopub.status.idle":"2021-08-20T21:06:26.770235Z","shell.execute_reply.started":"2021-08-20T21:06:22.021366Z","shell.execute_reply":"2021-08-20T21:06:26.768858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# проверим, что модель не ломается и проходит прямой проход\nsample = (next(iter(train_data_loader)))\nsample[1].shape","metadata":{"id":"dwhOvH3NREes","execution":{"iopub.status.busy":"2021-08-20T21:06:26.771833Z","iopub.execute_input":"2021-08-20T21:06:26.772184Z","iopub.status.idle":"2021-08-20T21:06:29.80951Z","shell.execute_reply.started":"2021-08-20T21:06:26.77215Z","shell.execute_reply":"2021-08-20T21:06:29.808761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = Umodel(sample[0].to(device))\nout.shape","metadata":{"id":"ecKcwXs6REes","execution":{"iopub.status.busy":"2021-08-20T21:06:29.81076Z","iopub.execute_input":"2021-08-20T21:06:29.811108Z","iopub.status.idle":"2021-08-20T21:06:30.74526Z","shell.execute_reply.started":"2021-08-20T21:06:29.811075Z","shell.execute_reply":"2021-08-20T21:06:30.744137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(out[0][2].detach().cpu());","metadata":{"id":"K3rLzIDxREet","execution":{"iopub.status.busy":"2021-08-20T21:06:30.746839Z","iopub.execute_input":"2021-08-20T21:06:30.747224Z","iopub.status.idle":"2021-08-20T21:06:30.996583Z","shell.execute_reply.started":"2021-08-20T21:06:30.747186Z","shell.execute_reply":"2021-08-20T21:06:30.995557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Что-то выходит, не ломается, значит всё ок.","metadata":{}},{"cell_type":"markdown","source":"Под обучением мы понимаем скармливание целевой функции оптимизирующей функции. Поэтому выберем оптимизирующую функцию и функцию потерь (целевая функция):","metadata":{"id":"eEV-tOLXREet"}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(Umodel.parameters())","metadata":{"id":"ETG0GVv_REet","execution":{"iopub.status.busy":"2021-08-20T21:06:31.000933Z","iopub.execute_input":"2021-08-20T21:06:31.001327Z","iopub.status.idle":"2021-08-20T21:06:31.007631Z","shell.execute_reply.started":"2021-08-20T21:06:31.001289Z","shell.execute_reply":"2021-08-20T21:06:31.00659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    # функция потерь на основе коэф. Dice\n    # в инициализации подтягиваем все методы реализованные в модуле\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    # переопределяем проход на логитах и целевых значениях\n    def forward(self, logits, targets):\n        # добавляем эпсилон = 1 чтобы не было деления на ноль\n        smooth = 1\n        # кол-во пришедших объектов в функцию потерь\n        num = targets.size(0)\n        probs = logits\n        m1 = probs.reshape(num, -1)\n        m2 = targets.reshape(num, -1)\n        intersection = (m1 * m2)\n        # коэф Dice чем больше тем лучше\n        score = (2. * intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)\n        # т.к. мы привыкли минимизировать функцию потерь, отнормируем сумму на кол-во\n        # объектов и вычтем из единицы\n        score = 1 - (score.sum() / num)\n        return score","metadata":{"id":"iTQV38d9REeu","execution":{"iopub.status.busy":"2021-08-20T21:06:31.009731Z","iopub.execute_input":"2021-08-20T21:06:31.010305Z","iopub.status.idle":"2021-08-20T21:06:31.018639Z","shell.execute_reply.started":"2021-08-20T21:06:31.010263Z","shell.execute_reply":"2021-08-20T21:06:31.017881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определим количество шагов внутри одной эпохи:","metadata":{"id":"rM6QCJD2REev"}},{"cell_type":"code","source":"total_steps = len(train_data_loader)\nprint(f\"{epochs} epochs, {total_steps} total_steps per epoch\")","metadata":{"id":"tt0NIHupREev","outputId":"1c6c5d5f-ed61-48d1-b417-2e43a618d224","execution":{"iopub.status.busy":"2021-08-20T21:06:31.020056Z","iopub.execute_input":"2021-08-20T21:06:31.020511Z","iopub.status.idle":"2021-08-20T21:06:31.033924Z","shell.execute_reply.started":"2021-08-20T21:06:31.02047Z","shell.execute_reply":"2021-08-20T21:06:31.033041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = DiceLoss()","metadata":{"id":"tV2_Lg-PREev","execution":{"iopub.status.busy":"2021-08-20T21:06:31.03535Z","iopub.execute_input":"2021-08-20T21:06:31.035805Z","iopub.status.idle":"2021-08-20T21:06:31.042693Z","shell.execute_reply.started":"2021-08-20T21:06:31.035768Z","shell.execute_reply":"2021-08-20T21:06:31.041612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Запускаем сам процесс обучения:","metadata":{"id":"nJbfGsGDREew"}},{"cell_type":"code","source":"#Импортируем библиотеку time для расчета, сколько времени у нас уходит на одну эпоху\nimport time\n\n\n# запускаем главный тренировочный цикл\nepoch_losses = []\nfor epoch in range(epochs):\n    # запоминаем время начала обучения\n    start_time = time.time()\n    epoch_loss = []\n    \n    for batch_idx, (data, labels) in enumerate(train_data_loader):\n        \n        data, labels = data.to(device), labels.to(device)        \n        \n        # обнуляем градиенты\n        optimizer.zero_grad()\n        # прогоняем данные через модель\n        outputs = Umodel(data)                \n        \n        # считаем ошибку\n        #loss = nn.CrossEntropyLoss(outputs,labels)# - torch.log(DiceLoss(outputs, labels))\n        loss = criterion(outputs, labels)\n        \n        # подсчёт градиента на обратном проходе\n        loss.backward()\n        \n        # шаг оптимизации, изменяя веса\n        optimizer.step()\n        \n        epoch_loss.append(loss.item())\n        \n        if batch_idx % 200 == 0:\n            print(f'batch index : {batch_idx} | loss : {loss.item()}')\n\n    print(f'Epoch {epoch+1}, loss: ', np.mean(epoch_loss))\n    end_time = time.time()\n    print(f'Spend time for 1 epoch: {end_time - start_time} sec')\n    \n    epoch_losses.append(epoch_loss)","metadata":{"id":"YR-Zd1M8REew","outputId":"cda76755-67ce-4efa-e1f6-876048efc2d5","execution":{"iopub.status.busy":"2021-08-20T21:06:31.044119Z","iopub.execute_input":"2021-08-20T21:06:31.044413Z","iopub.status.idle":"2021-08-20T21:37:02.382171Z","shell.execute_reply.started":"2021-08-20T21:06:31.044363Z","shell.execute_reply":"2021-08-20T21:37:02.380298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сохраним нашу модель:","metadata":{"id":"mNx_RKzCREex"}},{"cell_type":"code","source":"save_model_path = './Unet_Model_dice_loss.pth'","metadata":{"id":"DyLsM4SOREex","execution":{"iopub.status.busy":"2021-08-20T21:37:02.383341Z","iopub.status.idle":"2021-08-20T21:37:02.384063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# сохраняем веса через словарь\ntorch.save(Umodel.state_dict(), save_model_path)","metadata":{"id":"23G_GROXREex","execution":{"iopub.status.busy":"2021-08-20T21:37:02.385285Z","iopub.status.idle":"2021-08-20T21:37:02.386026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# сохраняем архитектуру\nnet = UNet(13).to(device)\nnet.load_state_dict(torch.load(save_model_path))","metadata":{"id":"V2ocd6X9REex","outputId":"2b4b5682-2f05-451b-997f-ffd56ef57337","execution":{"iopub.status.busy":"2021-08-20T21:37:02.387158Z","iopub.status.idle":"2021-08-20T21:37:02.38775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_orig(image):\n    image = image.permute(1, 2, 0)\n    image = image.numpy()\n    image = np.clip(image, 0, 1)\n    return image","metadata":{"id":"pKoNTKpzREey","execution":{"iopub.status.busy":"2021-08-20T21:37:02.388866Z","iopub.status.idle":"2021-08-20T21:37:02.38947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Проверим работу на тестовых объектах","metadata":{}},{"cell_type":"code","source":"class_idx = 1\n\nfor i, data in enumerate(test_data_loader):\n    images, labels = data\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = net(images)\n    f, axarr = plt.subplots(1,3, figsize=(15, 6))\n\n    for j in range(0, 4):\n        axarr[0].imshow(outputs.squeeze().detach().cpu().numpy()[j,class_idx,:,:])\n        axarr[0].set_title('Guessed labels')\n        axarr[1].imshow(labels.detach().cpu().numpy()[j,class_idx, :,:])\n        axarr[1].set_title('Ground truth labels')\n\n        original = get_orig(images[j].cpu())\n        axarr[2].imshow(original)\n        axarr[2].set_title('Original Images')\n        plt.show()\n    if i > 3:\n        break","metadata":{"id":"Q7DXd-ASREey","outputId":"98ba008d-151e-4d92-fef0-6b1a5d3cbd95","execution":{"iopub.status.busy":"2021-08-20T21:37:02.390532Z","iopub.status.idle":"2021-08-20T21:37:02.391224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Реализация на PyTorch\n\nСтоит сказать что уже **есть реализация Unet в PyTorch**. Она и другие популярные модели для решения задачи сегментации находятся в библиотеке [segmentation_models_pytorch](https://segmentation-modelspytorch.readthedocs.io/en/latest/index.html)","metadata":{"id":"UKeGoGmSREez"}},{"cell_type":"markdown","source":"Если у вас нет этой библиотеки, то для дальнейшей работы вам надо ее установить через pip","metadata":{"id":"YjGC7OZAREez"}},{"cell_type":"code","source":"!pip install segmentation-models-pytorch","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-20T21:37:02.392297Z","iopub.status.idle":"2021-08-20T21:37:02.39291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\n# создание модели\nBACKBONE = 'resnet34' # сеть для изучения признаков (предобученая)\nsegmodel = smp.Unet(BACKBONE, classes=13, activation='softmax').to(device) # сеть для сегментации\n# препроцессинг берём из imagenet, потому что она была предобучена на imagenet\npreprocess_input = smp.encoders.get_preprocessing_fn(BACKBONE, pretrained='imagenet')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.394081Z","iopub.status.idle":"2021-08-20T21:37:02.39466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# препроцессим, видим что картинка после препроцессинга стала темнее, но так лучше для сети\ndataset = SelfDrivingDataset(df, preprocessing=preprocess_input)\nimg, masks = dataset[0]\nprint(img.shape, masks.shape)\nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\nax[0].imshow(img.permute(1, 2, 0))\nax[1].imshow(masks.permute(1, 2, 0)[..., 10])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.395727Z","iopub.status.idle":"2021-08-20T21:37:02.396324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 70 % в тренировочную выборку, 30 - в тестовую\nX_train, X_test = train_test_split(df, test_size=0.3)\n\n# Упорядочиваем индексацию\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\n# Оборачиваем каждую выборку в наш кастомный датасет\ntrain_data = SelfDrivingDataset(X_train,\n                                preprocessing=preprocess_input)\ntest_data = SelfDrivingDataset(X_test,\n                               preprocessing=preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.397482Z","iopub.status.idle":"2021-08-20T21:37:02.398063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_loader = DataLoader(\n    train_data,\n    batch_size=8,\n    shuffle=True\n)\ntest_data_loader = DataLoader(\n    test_data,\n    batch_size=4,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.399175Z","iopub.status.idle":"2021-08-20T21:37:02.399769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img, target in train_data_loader:\n    print(img.shape, target.shape)\n    print(img[0].min(), img[0].max())\n    print(target[0].min(), target[0].max())\n    break","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.400909Z","iopub.status.idle":"2021-08-20T21:37:02.40153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dice loss и метрики есть готовые\ncriterion = smp.utils.losses.DiceLoss()\nmetrics = [smp.utils.metrics.IoU(),]\n\noptimizer = torch.optim.Adam(params=segmodel.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.402581Z","iopub.status.idle":"2021-08-20T21:37:02.403178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# цикл обучения тоже реализован\n# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    segmodel, \n    loss=criterion, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=device,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    segmodel, \n    loss=criterion, \n    metrics=metrics, \n    device=device,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.4043Z","iopub.status.idle":"2021-08-20T21:37:02.405013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# нужно написать только цикл по эпохам\n# train model\n\nmax_score = 0\n\nfor i in range(0, 1):\n    print(f'Epoch: {i + 1}')\n    train_logs = train_epoch.run(train_data_loader)\n    valid_logs = valid_epoch.run(test_data_loader)\n    \n    # сохраняем модель если метрика лучшая или что-то ещё (save model, change lr, etc.)\n    if max_score < valid_logs['iou_score']:\n        max_score = valid_logs['iou_score']\n        torch.save(segmodel, './best_model.pth')\n        print('Model saved!')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.406104Z","iopub.status.idle":"2021-08-20T21:37:02.406716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Видим, что за 1 эпоху переобучения нет. Можно пообучать подольше.\n\nПосмотрим как модель сегментирует.","metadata":{}},{"cell_type":"code","source":"class_idx = 1\n\nfor i, data in enumerate(test_data_loader):\n    images, labels = data\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = net(images)\n    f, axarr = plt.subplots(1,3, figsize=(15, 6))\n\n    for j in range(0, 4):\n        axarr[0].imshow(outputs.squeeze().detach().cpu().numpy()[j,class_idx,:,:])\n        axarr[0].set_title('Guessed labels')\n        axarr[1].imshow(labels.detach().cpu().numpy()[j,class_idx, :,:])\n        axarr[1].set_title('Ground truth labels')\n\n        original = get_orig(images[j].cpu())\n        axarr[2].imshow(original)\n        axarr[2].set_title('Original Images')\n        plt.show()\n    if i > 3:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-08-20T21:37:02.407782Z","iopub.status.idle":"2021-08-20T21:37:02.408426Z"},"trusted":true},"execution_count":null,"outputs":[]}]}