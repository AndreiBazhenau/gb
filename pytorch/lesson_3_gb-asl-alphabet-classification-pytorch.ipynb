{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Предобученные сети, которыми можно пользоваться\n\n## LeNet'98\n\nLeNet была создана ,чтобы распознавать цифры на почтовых бланках.\n\nОсобенности LeNet5:\n\n- Свёрточная нейросеть, использующая последовательность из трёх слоёв: слои свёртки (convolution), слои группирования (pooling) и слои нелинейности (non-linearity) –> с момента публикации работы Лекуна это, пожалуй, одна из главных особенностей глубокого обучения применительно к изображениям.\n- Подвыборка с использованием усреднения карт.\n- Нелинейность в виде гиперболического тангенса или сигмоид (проблема с затухающим градиентом).\n- Финальный классификатор в виде многослойной нейросети.\n\n<img src='https://drive.google.com/uc?exoprt=view&id=1pPe1aBh7ySg89cxbWEZ07iabvvXABUxd'>","metadata":{}},{"cell_type":"markdown","source":"Многие предобученные модели обучались на датасете ImageNet, который содержит 14,197,122 картинок - это набор данных размечанных изображений с высоким разрешением, относящихся примерно к 22 тысячам категорий. Проводился «Крупномасштабный конкурс визуального распознавания ImageNet» (ILSVRC2013). ILSVRC использует подмножество ImageNet из примерно 1000 изображений в каждой из 1000 категорий.\n\n<img src='https://avatars.mds.yandex.net/get-zen_doc/127510/pub_5c33ad37c906e200abbace3b_5c33adfbe5e73b00aad095a1/scale_1200'>","metadata":{}},{"cell_type":"markdown","source":"## AlexNet'12\n\nВ AlexNet результаты вычислений LeNet масштабированы в гораздо более крупную нейросеть, которая способна изучить намного более сложные объекты и их иерархии. Особенности:\n\n- Использование блоков ReLU в качестве нелинейностей (нет проблемы с затуханием градиентов).\n- Использование max pooling, что позволяет избежать эффектов усреднения average pooling.\n\nНа вход идут картинки 224х224, естественно не все картинки такого размера, поэтому будет достаточно просто сжать их до нужного размера.\n\n<img src='https://drive.google.com/uc?export=view&id=1sjEftFGiJ50-m3VevamktVznsx6bY3Yw' width=700>","metadata":{}},{"cell_type":"markdown","source":"## VGG'14\nВ разработанных в Оксфорде VGG-сетях в каждом свёрточном слое впервые применили фильтры 3х3 и объединили эти слои в последовательности свёрток.\n\nВместо применяемых в AlexNet фильтров 9х9 и 11х11 стали применять гораздо более мелкие фильтры, которых старались избежать авторы LeNet. Но большим преимуществом VGG стала находка, что несколько свёрток 3х3, объединённых в последовательность, могут эмулировать более крупные свертки, например, 5х5 или 7х7 (7х7 + 1 байес = 50 обучаемых параметров).\n\nКаскад из двух сверток 3х3 равен свертке 5х5, но с меньшим количеством параметров.\n(5х5 = 25 + 1 = 26; 3x3 + 3x3 + 2 = 20)\n\n<img src='https://drive.google.com/uc?export=view&id=1GvrtEDocJ3xp9RKqgQu0-JnyTssqZhzV'>","metadata":{}},{"cell_type":"markdown","source":"Глубокие сверточные нейронные сети превзошли человеческий уровень классификации изображений в 2015 году. Глубокие сети извлекают низко-, средне- и высокоуровневые признаки  сквозным многослойным способом, а увеличение количества слоев обогатить «уровни» признаков. Но у глубоких нейронных сетей была проблема: затухающие градиенты. Особенно это явно чувствуется с сигмоидой.\n\n$d\\sigma = \\sigma(1 - \\sigma) \\leqslant \\frac{1}{4}$\n\n<img src='https://drive.google.com/uc?export=view&id=171JbyNkSSqzhPdX4fp439zOouEzJmq_s'>\n\nЕсли постоянно брать производную сигмоиды, то максимальная производная сигмоиды - это 0,25. Если мы будем идти к началу чети, пто 0,25 будет возводиться в степень сколько у нас сигмоид. И это быстро становится близким к нулю. А если градиент равен нулю, то никакого обучения не будет.\n\nКроме того, что можно вместо сигмоиды брать ReLu, можно пробрасывать ошибку.","metadata":{}},{"cell_type":"markdown","source":"## GoogLeNet\n\nЭта сеть использует Inception блоки. Это параллельная комбинация свёрточных фильтров 1х1, 3х3 и 5х5. Но главная особенность заключается в использовании свёрточных блоков 1х1 для уменьшения количества каналов перед подачей в более «дорогие» сверточные блоки. Обычно эту часть называют bottleneck. Вместо использования свертки 5х5 на нашем изображении, можем сначала пройтись сверткой 1х1 уменьшив количество каналов, а затем по ним пройтись сверткой 5х5, вернув количество каналов. Операций будет меньше, а результат будет одинаковый.\n\n<img src='https://drive.google.com/uc?export=view&id=1hgoTi6d-pdRPHgnfVGssQIQXBUdrkWrk'>","metadata":{}},{"cell_type":"code","source":"!pip install torchsummary ","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:42:57.029553Z","iopub.execute_input":"2021-06-13T13:42:57.029808Z","iopub.status.idle":"2021-06-13T13:43:05.74811Z","shell.execute_reply.started":"2021-06-13T13:42:57.029785Z","shell.execute_reply":"2021-06-13T13:43:05.747224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchsummary import summary\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:05.751503Z","iopub.execute_input":"2021-06-13T13:43:05.751769Z","iopub.status.idle":"2021-06-13T13:43:06.939399Z","shell.execute_reply.started":"2021-06-13T13:43:05.751741Z","shell.execute_reply":"2021-06-13T13:43:06.938615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(\n    nn.Conv2d(in_channels=256,\n              out_channels=256,\n              kernel_size=5)\n)\n\nsummary(model.to(device), input_size=(256, 16, 16))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:06.941149Z","iopub.execute_input":"2021-06-13T13:43:06.941493Z","iopub.status.idle":"2021-06-13T13:43:12.143058Z","shell.execute_reply.started":"2021-06-13T13:43:06.941455Z","shell.execute_reply":"2021-06-13T13:43:12.142165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(\n    nn.Conv2d(in_channels=256,\n              out_channels=128,\n              kernel_size=1),\n    nn.Conv2d(in_channels=128,\n              out_channels=256,\n              kernel_size=5)\n)\n\nsummary(model.to(device), input_size=(256, 16, 16))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:12.144636Z","iopub.execute_input":"2021-06-13T13:43:12.145155Z","iopub.status.idle":"2021-06-13T13:43:12.1664Z","shell.execute_reply.started":"2021-06-13T13:43:12.145114Z","shell.execute_reply":"2021-06-13T13:43:12.165628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Разработчики этой сети придумали способ, чтобы градиенты не затухали: вводится несколько вспомогательных функций на протяжении всей сети, чтобы когда градиент от первого выхода начинал затухать, подключался градиент со второго выхода.\n\n<img src='https://drive.google.com/uc?export=view&id=1q3oJXpwGStYit5Ii13DIsexVqxwIjyjE'>","metadata":{}},{"cell_type":"markdown","source":"## ResNet'15\n\nResNet лучше всех борется с проблемами затухающих градиентов.\n\nДо этого боролись с затухающими градиентами только за счет ввода другой функции активации.\n\nЧтобы преодолеть проблему затухающих градиентов, Microsoft ввела глубокую «остаточную» структуру обучения.\n\n<img src='https://drive.google.com/uc?export=view&id=1RGJQl4-SmysYbAqwcy8Lm5qEPbiebZOO'>\n\nСмысл:\n\n$y = f(x) + x $<br>\n$dy = df(x) + 1 $<br>\n<h3>$\\frac{dL}{dx} = \\frac{dL}{dy} \\frac{dy}{dx} = \\frac{dL}{dy}(df(x) + 1 )$</h3><br>\n\nТо есть градиенты всё равно будут протекать дальше в немного измененном виде.\n\n\nСоединения быстрого доступа (shortcut connections, residual connections) пропускают один или несколько слоев и выполняют сопоставление идентификаторов.\n\n<img src='https://drive.google.com/uc?export=view&id=1JcQDIjA-97L2xs3o-JD4SWBld9J0OMW-'>\n\nЕще одна особенность ResNet, что в конце отсутствуют полносвязные слои, причина в том, что имеется уже и так довольно сложная нейронная сеть, в которой уже и так могла решиться задача, которая обычна перекладывается на полносвязный слой.","metadata":{}},{"cell_type":"markdown","source":"<img src='https://drive.google.com/uc?export=view&id=1qqbZ6iWZaD6LMjuIJ85mBGYpBwi0w-HL'>","metadata":{}},{"cell_type":"markdown","source":"## Распознование алфавита языка жестов","metadata":{}},{"cell_type":"code","source":"from torchvision import models, transforms, datasets\nimport torch\nfrom torch import nn\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:12.167532Z","iopub.execute_input":"2021-06-13T13:43:12.168034Z","iopub.status.idle":"2021-06-13T13:43:12.309507Z","shell.execute_reply.started":"2021-06-13T13:43:12.167997Z","shell.execute_reply":"2021-06-13T13:43:12.308732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_path = '../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train/'","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:12.310593Z","iopub.execute_input":"2021-06-13T13:43:12.311052Z","iopub.status.idle":"2021-06-13T13:43:12.314933Z","shell.execute_reply.started":"2021-06-13T13:43:12.311016Z","shell.execute_reply":"2021-06-13T13:43:12.314056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Определим трансформации для тестового и обучающего датасета.\n\n","metadata":{}},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize(224),\n#     transforms.RandomAffine(degrees=5, scale=(0.3, 1.1)), # аугументации\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:12.316163Z","iopub.execute_input":"2021-06-13T13:43:12.316645Z","iopub.status.idle":"2021-06-13T13:43:12.324103Z","shell.execute_reply.started":"2021-06-13T13:43:12.316611Z","shell.execute_reply":"2021-06-13T13:43:12.323365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Загрузим датасет и поделим его на обучение и валидацию.\n\n","metadata":{}},{"cell_type":"code","source":"%%time\n# данные разложены по папкам, берём из папок данные\ntrain_dataset = datasets.ImageFolder(train_data_path, transform=train_transforms)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:43:12.326191Z","iopub.execute_input":"2021-06-13T13:43:12.326798Z","iopub.status.idle":"2021-06-13T13:44:24.471051Z","shell.execute_reply.started":"2021-06-13T13:43:12.326766Z","shell.execute_reply":"2021-06-13T13:44:24.470214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nval_dataset = datasets.ImageFolder(train_data_path, transform=test_transforms)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:44:24.473755Z","iopub.execute_input":"2021-06-13T13:44:24.474012Z","iopub.status.idle":"2021-06-13T13:44:24.935799Z","shell.execute_reply.started":"2021-06-13T13:44:24.473986Z","shell.execute_reply":"2021-06-13T13:44:24.934958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(1)\nnum_train_samples = len(train_dataset)\n# num_train_samples = 20000\n\nval_split = 0.2\nsplit = int(num_train_samples * val_split)\n\n# перемешиваем индексы\nindices = torch.randperm(num_train_samples)\n\ntrain_subset = torch.utils.data.Subset(train_dataset, indices[split:])\nval_subset = torch.utils.data.Subset(val_dataset, indices[:split])\n\nlen(train_subset), len(val_subset)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:44:27.360685Z","iopub.execute_input":"2021-06-13T13:44:27.361008Z","iopub.status.idle":"2021-06-13T13:44:27.371892Z","shell.execute_reply.started":"2021-06-13T13:44:27.360977Z","shell.execute_reply":"2021-06-13T13:44:27.37107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    dataset=train_subset, \n    batch_size=batch_size,\n    shuffle=True\n)\n\nval_dataloader = torch.utils.data.DataLoader(\n    dataset=val_subset,\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:44:28.503495Z","iopub.execute_input":"2021-06-13T13:44:28.50381Z","iopub.status.idle":"2021-06-13T13:44:28.508395Z","shell.execute_reply.started":"2021-06-13T13:44:28.503784Z","shell.execute_reply":"2021-06-13T13:44:28.507344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = train_dataloader.dataset.dataset.classes","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:44:29.81828Z","iopub.execute_input":"2021-06-13T13:44:29.818678Z","iopub.status.idle":"2021-06-13T13:44:29.822822Z","shell.execute_reply.started":"2021-06-13T13:44:29.818649Z","shell.execute_reply":"2021-06-13T13:44:29.821675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Можем проверить, как работает dataloader.","metadata":{}},{"cell_type":"code","source":"for img, label in train_dataloader:\n    print(img.shape, label.shape)\n    print(f'Ground Truth {classes[label[0]]}')\n    plt.imshow(img[0].permute(1, 2, 0))\n    break","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:44:43.73258Z","iopub.execute_input":"2021-06-13T13:44:43.732922Z","iopub.status.idle":"2021-06-13T13:44:44.159856Z","shell.execute_reply.started":"2021-06-13T13:44:43.732887Z","shell.execute_reply":"2021-06-13T13:44:44.159108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet = models.resnet50(pretrained=True)\nsummary(resnet.to(device), input_size=(3, 224, 224))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-13T13:44:32.350703Z","iopub.execute_input":"2021-06-13T13:44:32.351027Z","iopub.status.idle":"2021-06-13T13:44:43.73093Z","shell.execute_reply.started":"2021-06-13T13:44:32.350998Z","shell.execute_reply":"2021-06-13T13:44:43.730124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Заморозим претренерованные слои, чтобы он не обучались.\n\n","metadata":{}},{"cell_type":"code","source":"for param in resnet.parameters():\n    param.requires_grad = False","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-13T13:44:55.286513Z","iopub.execute_input":"2021-06-13T13:44:55.286912Z","iopub.status.idle":"2021-06-13T13:44:55.296196Z","shell.execute_reply.started":"2021-06-13T13:44:55.286877Z","shell.execute_reply":"2021-06-13T13:44:55.295259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим кол-во параметров по слоям.","metadata":{}},{"cell_type":"code","source":"resnet","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-13T13:44:57.003437Z","iopub.execute_input":"2021-06-13T13:44:57.003751Z","iopub.status.idle":"2021-06-13T13:44:57.014741Z","shell.execute_reply.started":"2021-06-13T13:44:57.003721Z","shell.execute_reply":"2021-06-13T13:44:57.013866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Нам нужно переопределить последний слой т.к. у нас не 1000 классов как в ResNet.\n\nДля этого берём количество парамтеров, которое идёт на вход последнего линейного слоя и на выход даём количество классов и меняем в реснете на новый полносвязный слой.","metadata":{}},{"cell_type":"code","source":"in_features = resnet.fc.in_features\nfc = nn.Linear(in_features=in_features, out_features=len(classes))\nresnet.fc = fc\n\nsummary(resnet.to(device), input_size=(3, 224, 224))","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-13T13:45:14.764493Z","iopub.execute_input":"2021-06-13T13:45:14.764803Z","iopub.status.idle":"2021-06-13T13:45:14.85672Z","shell.execute_reply.started":"2021-06-13T13:45:14.764775Z","shell.execute_reply":"2021-06-13T13:45:14.855905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Теперь у нас на последнем слое не 1000, а 29 выходов и обучаем мы только последний слой.","metadata":{}},{"cell_type":"markdown","source":"Зададим функцию потерь и оптимизатор.","metadata":{}},{"cell_type":"code","source":"# функция потерь\ncriterion = nn.CrossEntropyLoss()\n\n# оптимизатор\noptimizer = torch.optim.Adam(resnet.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:45:29.349067Z","iopub.execute_input":"2021-06-13T13:45:29.349491Z","iopub.status.idle":"2021-06-13T13:45:29.356534Z","shell.execute_reply.started":"2021-06-13T13:45:29.349455Z","shell.execute_reply":"2021-06-13T13:45:29.3554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Обучаем модель.","metadata":{}},{"cell_type":"code","source":"from time import time\nfrom tqdm import tqdm\n\n\ndef train(model,\n          criterion,\n          optimizer,\n          train_dataloader,\n          test_dataloader,\n          print_every,\n          num_epoch):\n    steps = 0\n    train_losses, val_losses = [], []\n\n    model.to(device)\n    for epoch in tqdm(range(num_epoch)):\n        running_loss = 0\n        correct_train = 0\n        total_train = 0\n        start_time = time()\n        iter_time = time()\n        \n        model.train()\n        # проходимся по обучающему датасету\n        for i, (images, labels) in enumerate(train_dataloader):\n            steps += 1\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            # получаем предсказания\n            output = model(images)\n            # считаем ошибку\n            loss = criterion(output, labels)\n\n            # насколько корректно всё предсказали\n            correct_train += (torch.max(output, dim=1)[1] == labels).sum()\n            # сколько всего объектов было в предсказании\n            total_train += labels.size(0)\n\n            # Backward and optimize - обучение\n            # обнуляем градиенты\n            optimizer.zero_grad()\n            # считаем градиенты\n            loss.backward()\n            # меняем веса\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            # Logging\n            if steps % print_every == 0:\n                print(f'Epoch [{epoch + 1}]/[{num_epoch}]. Batch [{i + 1}]/[{len(train_dataloader)}].', end=' ')\n                print(f'Train loss {running_loss / steps:.3f}.', end=' ')\n                print(f'Train acc {correct_train / total_train * 100:.3f}.', end=' ')\n                \n                # считаем, что происходит на валидации (градиент не трогаем - так быстрее)\n                with torch.no_grad():\n                    # переводим модель в режим валидации\n                    model.eval()\n                    correct_val, total_val = 0, 0\n                    val_loss = 0\n                    # проходимся по тестовому набору данных\n                    for images, labels in test_dataloader:\n                        images = images.to(device)\n                        labels = labels.to(device)\n                        # предсказания\n                        output = model(images)\n                        loss = criterion(output, labels)\n                        val_loss += loss.item()\n\n                        correct_val += (torch.max(output, dim=1)[1] == labels).sum()\n                        total_val += labels.size(0)\n\n                print(f'Val loss {val_loss / len(test_dataloader):.3f}. Val acc {correct_val / total_val * 100:.3f}.', end=' ')\n                print(f'Took {time() - iter_time:.3f} seconds')\n                iter_time = time()\n\n                train_losses.append(running_loss / total_train)\n                val_losses.append(val_loss / total_val)\n\n\n        print(f'Epoch took {time() - start_time}') \n        torch.save(model, f'checkpoint_{correct_val / total_val * 100:.2f}')\n        \n    return model, train_losses, val_losses","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:45:31.2962Z","iopub.execute_input":"2021-06-13T13:45:31.296563Z","iopub.status.idle":"2021-06-13T13:45:31.312182Z","shell.execute_reply.started":"2021-06-13T13:45:31.296532Z","shell.execute_reply":"2021-06-13T13:45:31.311204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# печатаем каждые 50 итераций\nprint_every = 50\n# обучаем 2 эпохи (2 ч 15 мин)\nnum_epoch = 2\n\nresnet, train_losses, val_losses = train(\n    model=resnet,\n    criterion=criterion,\n    optimizer=optimizer,\n    train_dataloader=train_dataloader,\n    test_dataloader=val_dataloader,\n    print_every=print_every,\n    num_epoch=num_epoch\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T13:45:32.466861Z","iopub.execute_input":"2021-06-13T13:45:32.467188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(train_losses, label='Training loss')\nplt.plot(val_losses, label='Validation loss')\nplt.legend(frameon=False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Инференс\n\nПосмотрим как работает на тестовых картинках, на которых модель не обучалась.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom PIL import Image\n\n\ntest_data_path = Path('../input/asl-alphabet/asl_alphabet_test/asl_alphabet_test/')\n\n\nclass ASLTestDataset(torch.utils.data.Dataset):\n    def __init__(self, root_path, transforms=None):\n        super().__init__()\n        \n        # тестовы трансформации\n        self.transforms = transforms\n        # считываем все пути до картинок\n        self.imgs = sorted(list(Path(root_path).glob('*.jpg')))\n        \n    def __len__(self):\n        # сколько всего есть картинок\n        return len(self.imgs)\n    \n    def __getitem__(self, idx):\n        # получаем объекты по индексу\n        img_path = self.imgs[idx]\n        # открываем картинку\n        img = Image.open(img_path).convert('RGB')\n        # в названии картинки хранится класс, извекаем его с помощью сплита\n        label = img_path.parts[-1].split('_')[0]\n        if self.transforms:\n            # если есть трансформация - применяем её\n            img = self.transforms(img)\n        \n        # возвращаем картинку и класс\n        return img, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# инициализируем тестовый датасет с тестовыми трансформациями\ntest_dataset = ASLTestDataset(test_data_path, transforms=test_transforms)\n\ncolumns = 7\nrow = round(len(test_dataset) / columns)\n\nfig, ax = plt.subplots(row, columns, figsize=(columns * row, row * columns))\nplt.subplots_adjust(wspace=0.1, hspace=0.2)\n\ni, j = 0, 0\n# проходимся по тестовому датасету и визуализируем,\n# что предсказала модель и что должна была предсказать\nfor img, label in test_dataset:\n    img = torch.Tensor(img)\n    img = img.to(device)\n    resnet.eval()\n    prediction = resnet(img[None])\n\n    ax[i][j].imshow(img.cpu().permute(1, 2, 0))\n    ax[i][j].set_title(f'GT {label}. Pred {classes[torch.max(prediction, dim=1)[1]]}')\n    ax[i][j].axis('off')\n    j += 1\n    if j == columns:\n        j = 0\n        i += 1\n        \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}